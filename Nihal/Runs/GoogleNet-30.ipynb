{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8074811,"sourceType":"datasetVersion","datasetId":4765161}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.manifold import TSNE\nfrom torch.optim.lr_scheduler import StepLR\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T21:46:01.494117Z","iopub.execute_input":"2024-04-10T21:46:01.494486Z","iopub.status.idle":"2024-04-10T21:46:01.503373Z","shell.execute_reply.started":"2024-04-10T21:46:01.494456Z","shell.execute_reply":"2024-04-10T21:46:01.502079Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Transformations\n# We're transforming the images to 224x224, as that's the input size for GoogleNet. We're also normalizing the images.\n\n# Data Augmentation\n# We're using data augmentation to increase the size of the dataset. We're using the following transformations:\n# RandomHorizontalFlip\n# RandomRotation\n# RandomResizedCrop\n# ColorJitter\n# RandomAffine\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ColorJitter()\n])\n\nfull_dataset = datasets.ImageFolder('/kaggle/input/dataset-30/mammals', transform=transform)\nclass_counts = {class_name: 0 for class_name in full_dataset.classes}\nfor _, index in full_dataset.samples:\n    class_name = full_dataset.classes[index]\n    class_counts[class_name] += 1\nprint(\"Total number of classes:\", len(full_dataset.classes))\nprint(\"Class names:\", full_dataset.classes)\nprint(\"Number of images per class:\")\nfor class_name, count in class_counts.items():\n    print(f\" - {class_name}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T21:46:04.542418Z","iopub.execute_input":"2024-04-10T21:46:04.543124Z","iopub.status.idle":"2024-04-10T21:46:04.639534Z","shell.execute_reply.started":"2024-04-10T21:46:04.543089Z","shell.execute_reply":"2024-04-10T21:46:04.638564Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Total number of classes: 30\nClass names: ['african_elephant', 'alpaca', 'american_bison', 'anteater', 'arctic_fox', 'armadillo', 'baboon', 'badger', 'brown_bear', 'camel', 'giraffe', 'groundhog', 'highland_cattle', 'horse', 'jackal', 'kangaroo', 'koala', 'mongoose', 'mountain_goat', 'opossum', 'orangutan', 'polar_bear', 'porcupine', 'red_panda', 'rhinoceros', 'weasel', 'wildebeest', 'wombat', 'yak', 'zebra']\nNumber of images per class:\n - african_elephant: 347\n - alpaca: 333\n - american_bison: 343\n - anteater: 299\n - arctic_fox: 315\n - armadillo: 331\n - baboon: 330\n - badger: 310\n - brown_bear: 300\n - camel: 254\n - giraffe: 305\n - groundhog: 309\n - highland_cattle: 311\n - horse: 303\n - jackal: 278\n - kangaroo: 317\n - koala: 319\n - mongoose: 287\n - mountain_goat: 328\n - opossum: 330\n - orangutan: 340\n - polar_bear: 356\n - porcupine: 321\n - red_panda: 329\n - rhinoceros: 274\n - weasel: 282\n - wildebeest: 307\n - wombat: 315\n - yak: 254\n - zebra: 272\n","output_type":"stream"}]},{"cell_type":"code","source":"train_size = int(0.7 * len(full_dataset))\ntest_validation_size = len(full_dataset) - train_size\nvalidation_size = test_validation_size // 2\ntest_size = test_validation_size - validation_size\n\ntrain_dataset, test_validation_dataset = random_split(full_dataset, [train_size, test_validation_size])\nvalidation_dataset, test_dataset = random_split(test_validation_dataset, [validation_size, test_size])\n\nprint(\"Size of the entire Dataset: \", len(full_dataset))\nprint(\"Size of the training Dataset: \", len(train_dataset))\nprint(\"Size of the validation Dataset: \", len(validation_dataset))\nprint(\"Size of the test Dataset: \", len(test_dataset))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:35:14.498183Z","iopub.execute_input":"2024-04-10T22:35:14.498584Z","iopub.status.idle":"2024-04-10T22:35:14.509541Z","shell.execute_reply.started":"2024-04-10T22:35:14.498552Z","shell.execute_reply":"2024-04-10T22:35:14.508277Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Size of the entire Dataset:  9299\nSize of the training Dataset:  6509\nSize of the validation Dataset:  1395\nSize of the test Dataset:  1395\n","output_type":"stream"}]},{"cell_type":"code","source":"googlenet = models.googlenet(pretrained=False, aux_logits=False)\ngooglenet.fc = torch.nn.Linear(googlenet.fc.in_features, 30)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngooglenet.to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(googlenet.parameters(), lr=0.001, momentum=0.9)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\nwriter = SummaryWriter()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:35:17.716851Z","iopub.execute_input":"2024-04-10T22:35:17.717620Z","iopub.status.idle":"2024-04-10T22:35:17.934231Z","shell.execute_reply.started":"2024-04-10T22:35:17.717586Z","shell.execute_reply":"2024-04-10T22:35:17.933200Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Here, the training section begins. Let me first define the first 4 variables:\n\n* Patience: The number of epochs to wait before stopping the training if the validation loss doesn't decrease.\n* Best Validation Loss: The best validation loss we've seen so far.\n* Patience Counter: The number of epochs we've waited so far.\n* Epochs: The number of epochs we're training for. After this the training loop begins, in which the model is training. We're printing ALL the metrics (Training Loss, Validation Loss, Training Accuracy, Validation Accuracy) after every epoch, so that we know where we're heading with each epoch.","metadata":{}},{"cell_type":"code","source":"patience = 5\nbest_val_loss = np.inf\npatience_counter = 0\nepochs = 25\nfor epoch in range(epochs):\n    epoch_start_time = time.time()\n    train_loss = 0.0\n    train_correct = 0\n    total_train = 0\n    googlenet.train()\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = googlenet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    train_accuracy = 100 * train_correct / total_train\n    writer.add_scalar('Loss/Train', train_loss / len(train_loader), epoch)\n    writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n\n    googlenet.eval()\n    val_loss = 0.0\n    val_correct = 0\n    total_val = 0\n    with torch.no_grad():\n        for images, labels in validation_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = googlenet(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * val_correct / total_val\n    writer.add_scalar('Loss/Validation', val_loss / len(validation_loader), epoch)\n    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n\n    if val_loss / len(validation_loader) < best_val_loss:\n        best_val_loss = val_loss / len(validation_loader)\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1}: Train Loss: {train_loss / len(train_loader):.4f}, \"\n      f\"Train Accuracy: {train_accuracy:.2f}%, \"\n      f\"Val Loss: {val_loss / len(validation_loader):.4f}, \"\n      f\"Val Accuracy: {val_accuracy:.2f}%\")\nwriter.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:35:20.285909Z","iopub.execute_input":"2024-04-10T22:35:20.286530Z","iopub.status.idle":"2024-04-10T22:51:46.211598Z","shell.execute_reply.started":"2024-04-10T22:35:20.286498Z","shell.execute_reply":"2024-04-10T22:51:46.210538Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss: 3.0493, Train Accuracy: 14.58%, Val Loss: 2.8152, Val Accuracy: 21.58%\nEpoch 2: Train Loss: 2.5208, Train Accuracy: 28.74%, Val Loss: 2.9388, Val Accuracy: 23.73%\nEpoch 3: Train Loss: 2.2600, Train Accuracy: 34.48%, Val Loss: 2.4368, Val Accuracy: 32.04%\nEpoch 4: Train Loss: 2.0547, Train Accuracy: 40.74%, Val Loss: 2.2539, Val Accuracy: 37.99%\nEpoch 5: Train Loss: 1.8810, Train Accuracy: 46.32%, Val Loss: 2.2110, Val Accuracy: 40.14%\nEpoch 6: Train Loss: 1.5657, Train Accuracy: 55.17%, Val Loss: 1.6168, Val Accuracy: 53.98%\nEpoch 7: Train Loss: 1.4612, Train Accuracy: 58.41%, Val Loss: 1.5954, Val Accuracy: 55.70%\nEpoch 8: Train Loss: 1.3937, Train Accuracy: 60.44%, Val Loss: 1.5466, Val Accuracy: 56.13%\nEpoch 9: Train Loss: 1.3463, Train Accuracy: 61.75%, Val Loss: 1.5314, Val Accuracy: 56.49%\nEpoch 10: Train Loss: 1.3058, Train Accuracy: 62.84%, Val Loss: 1.4862, Val Accuracy: 58.14%\nEpoch 11: Train Loss: 1.2431, Train Accuracy: 65.54%, Val Loss: 1.4455, Val Accuracy: 59.00%\nEpoch 12: Train Loss: 1.2221, Train Accuracy: 66.32%, Val Loss: 1.4520, Val Accuracy: 58.85%\nEpoch 13: Train Loss: 1.2170, Train Accuracy: 66.15%, Val Loss: 1.4350, Val Accuracy: 59.71%\nEpoch 14: Train Loss: 1.2102, Train Accuracy: 66.45%, Val Loss: 1.4270, Val Accuracy: 58.49%\nEpoch 15: Train Loss: 1.1960, Train Accuracy: 66.97%, Val Loss: 1.4154, Val Accuracy: 60.36%\nEpoch 16: Train Loss: 1.1770, Train Accuracy: 68.09%, Val Loss: 1.4118, Val Accuracy: 59.93%\nEpoch 17: Train Loss: 1.1943, Train Accuracy: 66.86%, Val Loss: 1.4309, Val Accuracy: 59.93%\nEpoch 18: Train Loss: 1.1976, Train Accuracy: 67.00%, Val Loss: 1.4152, Val Accuracy: 59.71%\nEpoch 19: Train Loss: 1.1874, Train Accuracy: 67.52%, Val Loss: 1.4172, Val Accuracy: 59.00%\nEpoch 20: Train Loss: 1.1953, Train Accuracy: 67.01%, Val Loss: 1.4299, Val Accuracy: 59.86%\nEarly stopping triggered at epoch 21\n","output_type":"stream"}]}]}